# Inference Large language models with vLLM
This repo implements vLLM to self-hosted LLM on your own local.

## Installation
```sh
pip install -r requirements.txt
```

## Execution
- Select your own LLM and put it into LLM_MODEL="" in .env
```python
python inference/main.py
```

## Future plans
- TBD
